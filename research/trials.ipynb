{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a215b65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52335e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract data from pdf file\n",
    "\n",
    "def load_pdf_file(data):\n",
    "    loader = DirectoryLoader(data, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Tag each document with the PDF filename it came from\n",
    "    for doc in documents:\n",
    "        file_path = doc.metadata.get(\"source\", \"\")\n",
    "        doc.metadata[\"source\"] = os.path.basename(file_path)\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68afb3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books loaded: {'Medical_book5.pdf', 'Medical_book4.pdf', 'Medical_book2.pdf', 'Medical_book1.pdf', 'Medical_book3.pdf'}\n"
     ]
    }
   ],
   "source": [
    "all_docs = load_pdf_file(\"Data/\")  # folder containing book1.pdf and book2.pdf\n",
    "\n",
    "sources = set([doc.metadata[\"source\"] for doc in all_docs])\n",
    "print(\"Books loaded:\", sources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c90e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NEETHEESWARAN\\Git2\\Medibot-demo\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "#extracted_data = load_pdf_file(data=\"c:/Users/NEETHEESWARAN/Git2/Medical-chatbot-Gen-AI/Data/\")\n",
    "\n",
    "extracted_data = load_pdf_file(data=\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc53088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into smaller chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7def7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 38122\n"
     ]
    }
   ],
   "source": [
    "text_chunks = text_split(extracted_data)\n",
    "print(f\"Total number of chunks: {len(text_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee506582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "#Download embeddings from HuggingFace\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5057ff56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of query result: 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hi\")\n",
    "print(\"length of query result:\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d5c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()  # By default looks for .env in the current directory\n",
    "\n",
    "# Now retrieve the key\n",
    "api_key = os.environ.get('PINECONE_API_KEY')\n",
    "#openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "groq_api_key= os.environ.get('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25f2086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Admin', 'AwsRegion', 'AzureRegion', 'BackupList', 'BackupModel', 'ByocSpec', 'CloudProvider', 'CollectionDescription', 'CollectionList', 'Config', 'ConfigBuilder', 'ConfigureIndexEmbed', 'CreateIndexForModelEmbedTypedDict', 'DeleteRequest', 'DeletionProtection', 'DescribeIndexStatsRequest', 'DescribeIndexStatsResponse', 'EmbedModel', 'EmbeddingsList', 'FetchResponse', 'ForbiddenException', 'GcpRegion', 'ImportErrorMode', 'IndexEmbed', 'IndexList', 'IndexModel', 'ListConversionException', 'Metric', 'ModelInfo', 'ModelInfoList', 'NamespaceDescription', 'NotFoundException', 'Pinecone', 'PineconeApiAttributeError', 'PineconeApiException', 'PineconeApiKeyError', 'PineconeApiTypeError', 'PineconeApiValueError', 'PineconeAsyncio', 'PineconeConfig', 'PineconeConfigurationError', 'PineconeException', 'PineconeProtocolError', 'PodIndexEnvironment', 'PodSpec', 'PodSpecDefinition', 'PodType', 'QueryRequest', 'QueryResponse', 'RerankModel', 'RerankResult', 'RestoreJobList', 'RestoreJobModel', 'RpcStatus', 'ScoredVector', 'SearchQuery', 'SearchQueryVector', 'SearchRerank', 'ServerlessSpec', 'ServerlessSpecDefinition', 'ServiceException', 'SingleQueryResults', 'SparseValues', 'SparseValuesDictionaryExpectedError', 'SparseValuesMissingKeysError', 'SparseValuesTypeError', 'UnauthorizedException', 'UpdateRequest', 'UpsertResponse', 'Vector', 'VectorDictionaryExcessKeysError', 'VectorDictionaryMissingKeysError', 'VectorTupleLengthError', 'VectorType', '_LAZY_IMPORTS', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_check_for_deprecated_plugins', '_config_lazy_imports', '_db_control_lazy_imports', '_db_data_lazy_imports', '_inference_lazy_imports', '_setup_lazy_imports', 'admin', 'config', 'configure_index', 'core', 'create_collection', 'create_index', 'delete_collection', 'delete_index', 'deprecated_plugins', 'deprecation_warnings', 'describe_collection', 'describe_index', 'exceptions', 'init', 'langchain_import_warnings', 'legacy_pinecone_interface', 'list_collections', 'list_indexes', 'logging', 'openapi_support', 'pinecone', 'pinecone_asyncio', 'pinecone_interface_asyncio', 'scale_index', 'utils']\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "print(dir(pinecone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274c4d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "\n",
    "pc = Pinecone(api_key=api_key)\n",
    "\n",
    "index_name = \"medicalbot\"\n",
    "\n",
    "\n",
    "# pc.create_index(\n",
    "#     name=index_name,\n",
    "#     dimension=384, \n",
    "#     metric=\"cosine\", \n",
    "#     spec=ServerlessSpec(\n",
    "#         cloud=\"aws\", \n",
    "#         region=\"us-east-1\"\n",
    "#     ) \n",
    "# ) \n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91971665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = api_key\n",
    "os.environ[\"GROQ_API_KEY\"] = groq_api_key\n",
    "#os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c200bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embed and upload the data to Pinecone\n",
    "# If this is your first time adding documents\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4de6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load existing index\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each document and upload to the existing index\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name = index_name,\n",
    "    embedding = embeddings\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9479f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\",search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c94f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e132d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(temperature=0.7, model_name=\"llama3-70b-8192\") #mixtral-8x7b-32768\")  # or "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1321d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a knowledgeable and trustworthy AI medical assistant. \"\n",
    "    \"Always prioritize answering the user’s question using information from the retrieved medical book or trusted internal knowledge. \"\n",
    "    \"If the book content does not contain a clear or complete answer, use available tools to search for up-to-date medical information. \"\n",
    "    \"When using external sources, always start with: \"\n",
    "    \"'Note: The following answer is based on external information retrieved from [source_name].' \"\n",
    "    \"Then provide a clear, accurate, and helpful explanation. \"\n",
    "    \"Avoid vague language, and do not mention the word 'context' or reference your training data. \"\n",
    "    \"Your responses should be medically sound, professional, and empathetic, written in 4–6 well-structured sentences.\"\n",
    ")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8e0b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NEETHEESWARAN\\AppData\\Local\\Temp\\ipykernel_26788\\3491887695.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# 1. Create memory object\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", \n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# 2. Create conversational chain with memory\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    verbose=True  # Optional: helpful for debugging\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de054072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is Acne?\n",
      "Assistant: Acne is a common skin disease characterized by pimples on the face, chest, and back. It occurs when the pores of the skin become clogged with oil, dead skin cells, and bacteria.\n",
      "Follow Up Input: What is Pneumonia?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Pneumonia\n",
      "Definition\n",
      "Pneumonia is an infection of the lung, and can be\n",
      "caused by nearly any class of organism known to cause\n",
      "human infections. These include bacteria, viruses, fungi,\n",
      "and parasites. In the United States, pneumonia is the sixth\n",
      "most common disease leading to death. It is also the most\n",
      "common fatal infection acquired by already hospitalized\n",
      "patients. In developing countries, pneumonia ties with\n",
      "diarrhea as the most common cause of death.\n",
      "Description\n",
      "Anatomy of the lung\n",
      "\n",
      "how pneumonia is defined.\n",
      "Those people most at risk of developing pneumo-\n",
      "coccal pneumonia have a weakened immune system.\n",
      "This includes the elderly, infants, cancer patients, AIDS\n",
      "patients, post-operative patients, alcoholics, and those\n",
      "with diabetes. Pneumococcal pneumonia is a disease that\n",
      "has a high rate of hospital transmission, putting hospital\n",
      "patients at greater risk. Prior lung infections also makes\n",
      "someone more likely to develop pneumococcal pneumo-\n",
      "\n",
      "GALE ENCYCLOPEDIA OF MEDICINE 2 2635\n",
      "Pneumocystis pneumonia\n",
      "Human: What is Pneumonia?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Pneumonia is an infection of the lung that can be caused by nearly any class of organism known to cause human infections, including bacteria, viruses, fungi, and parasites.\n"
     ]
    }
   ],
   "source": [
    "response = qa_chain.invoke({\"question\": \"What is Pneumonia?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c047aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = invoke_rag_chain_with_retry(\"What is Acne?\")\n",
    "print(response[\"answer\"] if isinstance(response, dict) else response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI-Medical-LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
